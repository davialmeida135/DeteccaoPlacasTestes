{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HuEs3NEUOrkL",
        "outputId": "ba6fe466-2d20-4999-d8d0-ca79b5fb851d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bounding Box Coordinates: tensor([[[[[0.4040, 0.4500, 0.5655, 0.4082]],\n",
            "\n",
            "          [[0.6181, 0.7294, 0.6103, 0.5124]],\n",
            "\n",
            "          [[0.6302, 0.4698, 0.5376, 0.4066]],\n",
            "\n",
            "          [[0.7269, 0.3208, 0.5704, 0.5425]],\n",
            "\n",
            "          [[0.3405, 0.6155, 0.5017, 0.2718]],\n",
            "\n",
            "          [[0.6383, 0.5059, 0.5659, 0.3284]],\n",
            "\n",
            "          [[0.3924, 0.4908, 0.4499, 0.4931]]],\n",
            "\n",
            "\n",
            "         [[[0.5650, 0.7490, 0.6087, 0.2458]],\n",
            "\n",
            "          [[0.6082, 0.4122, 0.2490, 0.5270]],\n",
            "\n",
            "          [[0.4823, 0.4570, 0.6441, 0.4650]],\n",
            "\n",
            "          [[0.5130, 0.3953, 0.6709, 0.4198]],\n",
            "\n",
            "          [[0.4355, 0.3837, 0.7253, 0.2568]],\n",
            "\n",
            "          [[0.6370, 0.2580, 0.6187, 0.3856]],\n",
            "\n",
            "          [[0.5814, 0.6404, 0.5980, 0.7349]]],\n",
            "\n",
            "\n",
            "         [[[0.6619, 0.3949, 0.5487, 0.4469]],\n",
            "\n",
            "          [[0.4373, 0.5052, 0.3910, 0.3630]],\n",
            "\n",
            "          [[0.5558, 0.4774, 0.4815, 0.6201]],\n",
            "\n",
            "          [[0.4652, 0.4201, 0.4570, 0.5788]],\n",
            "\n",
            "          [[0.2039, 0.6195, 0.7260, 0.4164]],\n",
            "\n",
            "          [[0.4998, 0.3866, 0.1973, 0.5877]],\n",
            "\n",
            "          [[0.6599, 0.4525, 0.3876, 0.5940]]],\n",
            "\n",
            "\n",
            "         [[[0.4069, 0.6730, 0.6877, 0.5766]],\n",
            "\n",
            "          [[0.4493, 0.4947, 0.5593, 0.3861]],\n",
            "\n",
            "          [[0.5351, 0.5987, 0.6122, 0.4578]],\n",
            "\n",
            "          [[0.7512, 0.6696, 0.6202, 0.3689]],\n",
            "\n",
            "          [[0.5235, 0.6094, 0.3918, 0.5254]],\n",
            "\n",
            "          [[0.5001, 0.3700, 0.5592, 0.2775]],\n",
            "\n",
            "          [[0.4242, 0.4271, 0.5547, 0.3078]]],\n",
            "\n",
            "\n",
            "         [[[0.3703, 0.7684, 0.4508, 0.4624]],\n",
            "\n",
            "          [[0.4967, 0.5052, 0.7352, 0.4732]],\n",
            "\n",
            "          [[0.5046, 0.4422, 0.4684, 0.5819]],\n",
            "\n",
            "          [[0.5753, 0.4502, 0.4775, 0.4532]],\n",
            "\n",
            "          [[0.5204, 0.7053, 0.5158, 0.4741]],\n",
            "\n",
            "          [[0.4325, 0.3130, 0.7478, 0.6047]],\n",
            "\n",
            "          [[0.4269, 0.3682, 0.3836, 0.4345]]],\n",
            "\n",
            "\n",
            "         [[[0.4621, 0.6050, 0.3061, 0.5250]],\n",
            "\n",
            "          [[0.4571, 0.6038, 0.5487, 0.6421]],\n",
            "\n",
            "          [[0.7449, 0.5529, 0.3835, 0.5166]],\n",
            "\n",
            "          [[0.5081, 0.4532, 0.2686, 0.3758]],\n",
            "\n",
            "          [[0.3242, 0.5288, 0.3567, 0.3566]],\n",
            "\n",
            "          [[0.4278, 0.3891, 0.3178, 0.4756]],\n",
            "\n",
            "          [[0.4824, 0.7255, 0.4232, 0.6945]]],\n",
            "\n",
            "\n",
            "         [[[0.5446, 0.4611, 0.3782, 0.3656]],\n",
            "\n",
            "          [[0.4939, 0.6729, 0.3439, 0.4504]],\n",
            "\n",
            "          [[0.2618, 0.4338, 0.5029, 0.5879]],\n",
            "\n",
            "          [[0.3756, 0.6628, 0.2456, 0.3544]],\n",
            "\n",
            "          [[0.4781, 0.4132, 0.5977, 0.3951]],\n",
            "\n",
            "          [[0.6083, 0.3759, 0.6079, 0.3557]],\n",
            "\n",
            "          [[0.4093, 0.5887, 0.3151, 0.7345]]]]], grad_fn=<ViewBackward0>)\n",
            "Class Scores: tensor([[[[[-4.8320e-01, -6.5053e-01, -6.5320e-01,  ..., -3.6018e-01,\n",
            "            -6.7151e-01, -4.8033e-01]],\n",
            "\n",
            "          [[ 7.2006e-02, -5.7305e-02, -2.9160e-02,  ..., -3.2148e-01,\n",
            "            -1.7980e-01, -3.8403e-01]],\n",
            "\n",
            "          [[ 3.1394e-02,  4.8640e-01,  4.9136e-01,  ..., -4.3140e-01,\n",
            "             1.2077e-01, -2.4285e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-3.1904e-01,  2.3012e-01,  3.9211e-01,  ..., -2.3360e-01,\n",
            "             1.7373e+00, -1.2213e-02]],\n",
            "\n",
            "          [[-4.9288e-01,  3.1393e-01,  6.2995e-01,  ..., -1.8559e-02,\n",
            "             5.4472e-01, -5.0963e-02]],\n",
            "\n",
            "          [[-7.4952e-01, -5.6263e-01, -1.5929e-01,  ..., -1.6971e-01,\n",
            "             3.5817e-02,  9.8620e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 9.0543e-02, -5.0157e-01, -2.7548e-01,  ..., -1.4685e-01,\n",
            "            -5.4169e-01,  6.7497e-01]],\n",
            "\n",
            "          [[ 9.2588e-01, -3.2598e-01, -2.2987e-01,  ..., -5.1813e-01,\n",
            "            -8.1316e-01, -2.7508e-01]],\n",
            "\n",
            "          [[ 3.8103e-01,  3.7846e-01, -4.9958e-01,  ..., -4.7233e-01,\n",
            "             3.4246e-01, -6.6943e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-4.5097e-01, -4.1479e-01, -2.5103e-01,  ..., -6.2098e-01,\n",
            "             7.8705e-02,  1.2796e-02]],\n",
            "\n",
            "          [[ 3.2780e-01, -1.6319e-01, -2.0683e-01,  ..., -1.7343e-01,\n",
            "            -7.0026e-01, -1.3370e-01]],\n",
            "\n",
            "          [[ 4.3962e-01,  1.1040e+00,  2.5875e-01,  ..., -6.2862e-01,\n",
            "            -2.7013e-01, -4.9505e-01]]],\n",
            "\n",
            "\n",
            "         [[[-8.6430e-01, -5.8058e-01,  3.0270e-01,  ...,  2.8194e-04,\n",
            "             1.1781e+00, -8.8602e-01]],\n",
            "\n",
            "          [[ 3.7176e-01, -4.0823e-01, -5.5097e-02,  ..., -4.0294e-01,\n",
            "             2.0140e-01, -1.6306e-01]],\n",
            "\n",
            "          [[ 3.8145e-01,  8.4671e-01,  1.5304e-01,  ...,  1.1185e-01,\n",
            "            -3.0392e-01, -4.4429e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-6.6951e-02, -1.3282e-01,  6.7296e-01,  ..., -5.1309e-01,\n",
            "             6.3251e-01, -2.6037e-01]],\n",
            "\n",
            "          [[ 7.0090e-01,  8.2225e-01,  3.8506e-01,  ...,  4.6121e-01,\n",
            "             2.5474e-03,  6.6492e-02]],\n",
            "\n",
            "          [[ 6.1379e-01, -4.1467e-01,  8.5482e-01,  ..., -4.4998e-01,\n",
            "             2.6144e-01,  1.1379e-02]]],\n",
            "\n",
            "\n",
            "         ...,\n",
            "\n",
            "\n",
            "         [[[-3.6823e-01,  2.6150e-01, -6.9850e-01,  ...,  5.2892e-01,\n",
            "            -1.3976e-01,  4.6692e-01]],\n",
            "\n",
            "          [[-2.2062e-01, -6.4016e-01, -3.2409e-01,  ...,  3.2018e-01,\n",
            "            -7.2719e-02,  5.5003e-01]],\n",
            "\n",
            "          [[ 7.5521e-01,  4.3062e-02, -9.1999e-01,  ...,  7.3006e-01,\n",
            "            -2.7579e-02,  8.0535e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[-2.1001e-01, -1.5876e-01, -9.9070e-01,  ...,  6.7588e-02,\n",
            "            -2.2461e-01, -3.8811e-01]],\n",
            "\n",
            "          [[-9.9398e-03, -1.3470e+00, -3.8740e-01,  ...,  2.0623e-01,\n",
            "             7.4870e-01, -4.2886e-01]],\n",
            "\n",
            "          [[ 2.2677e-01, -6.2826e-01,  3.7546e-01,  ...,  2.5418e-01,\n",
            "            -8.8050e-01,  1.2985e-01]]],\n",
            "\n",
            "\n",
            "         [[[-7.2467e-01, -4.5789e-01, -3.0954e-01,  ..., -9.5745e-01,\n",
            "             5.2854e-01,  1.2382e+00]],\n",
            "\n",
            "          [[-1.6708e+00,  3.1201e-01,  1.3075e-01,  ..., -3.0136e-01,\n",
            "             3.0285e-01, -2.0621e-01]],\n",
            "\n",
            "          [[-5.3016e-01, -4.7777e-02, -4.0277e-01,  ..., -2.8217e-01,\n",
            "             4.1031e-01, -5.7451e-01]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 8.7093e-01, -5.6464e-01, -1.0654e-01,  ..., -1.3097e+00,\n",
            "             6.4439e-02,  2.4891e-01]],\n",
            "\n",
            "          [[ 2.5971e-01, -3.5856e-01, -5.2207e-01,  ...,  3.2198e-01,\n",
            "             1.2857e+00, -4.8838e-01]],\n",
            "\n",
            "          [[ 2.6283e-01, -2.6249e-01,  7.0848e-01,  ...,  1.4522e-01,\n",
            "            -2.9533e-02,  7.0028e-01]]],\n",
            "\n",
            "\n",
            "         [[[ 5.5625e-01,  4.9955e-01,  8.5557e-01,  ...,  1.4580e-02,\n",
            "            -2.9678e-01, -8.3687e-02]],\n",
            "\n",
            "          [[ 1.8609e-02, -1.3470e+00,  5.3092e-01,  ..., -2.7005e-01,\n",
            "             7.7660e-01,  5.9812e-01]],\n",
            "\n",
            "          [[ 4.7314e-01,  8.7479e-01,  5.3736e-01,  ...,  5.6406e-01,\n",
            "             1.1590e-01, -5.4746e-02]],\n",
            "\n",
            "          ...,\n",
            "\n",
            "          [[ 9.8493e-01, -3.3022e-01, -3.7027e-01,  ..., -9.1857e-02,\n",
            "             4.7342e-01,  4.9462e-01]],\n",
            "\n",
            "          [[-5.1401e-01, -1.5095e-01, -2.4733e-01,  ..., -8.1417e-02,\n",
            "            -1.4696e-01, -3.0130e-01]],\n",
            "\n",
            "          [[ 6.0171e-01,  9.4744e-01, -1.5681e-01,  ...,  4.2254e-01,\n",
            "             6.1439e-01, -8.8610e-01]]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# prompt: implement a resnet convolutional network with pytorch for detecting boudning boxes and defining classes on images\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNetBoundingBoxDetector(nn.Module):\n",
        "    def __init__(self, num_classes, grid_size=7, num_anchors=1):\n",
        "        super(ResNetBoundingBoxDetector, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)  # Use a pretrained ResNet\n",
        "        num_features = self.resnet.fc.in_features\n",
        "\n",
        "        # Replace the ResNet's FC layer with custom outputs\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        # Bounding box regression: 4 coords (x, y, w, h) per anchor per grid cell\n",
        "        self.fc_bbox = nn.Linear(num_features, grid_size * grid_size * num_anchors * 4)\n",
        "\n",
        "        # Class prediction: num_classes per anchor per grid cell\n",
        "        self.fc_class = nn.Linear(num_features, grid_size * grid_size * num_anchors * num_classes)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()  # To normalize bounding box coordinates (0-1)\n",
        "\n",
        "        self.grid_size = grid_size\n",
        "        self.num_anchors = num_anchors\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        x = self.resnet(x)\n",
        "\n",
        "        # Bounding box coordinates\n",
        "        bbox_coords = self.sigmoid(self.fc_bbox(x))\n",
        "        bbox_coords = bbox_coords.view(-1, self.grid_size, self.grid_size, self.num_anchors, 4)\n",
        "\n",
        "        # Class scores\n",
        "        class_scores = self.fc_class(x)\n",
        "        class_scores = class_scores.view(-1, self.grid_size, self.grid_size, self.num_anchors, self.num_classes)\n",
        "\n",
        "        return bbox_coords, class_scores\n",
        "\n",
        "# Example usage:\n",
        "num_classes = 10  # Number of classes\n",
        "num_boxes = 1 # Number of bounding boxes per image\n",
        "\n",
        "model = ResNetBoundingBoxDetector(36)\n",
        "\n",
        "\n",
        "# Example input (replace with your actual image data)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Perform a forward pass\n",
        "bbox_coords, class_scores = model(dummy_input)\n",
        "\n",
        "\n",
        "print(\"Bounding Box Coordinates:\", bbox_coords)\n",
        "print(\"Class Scores:\", class_scores)\n",
        "\n",
        "# Define loss functions and optimizer\n",
        "# ... (add your loss function and optimizer here)\n",
        "# Example:\n",
        "# criterion_bbox = nn.MSELoss() # or another suitable loss function\n",
        "# criterion_class = nn.CrossEntropyLoss() # assuming class_scores are not probabilities\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "# ... (add your training loop here)\n",
        "\n",
        "# Evaluation\n",
        "# ... (add your evaluation metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w2-cs3hMWJnR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, img_dir, annot_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.annot_dir = annot_dir\n",
        "        self.transform = transform\n",
        "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
        "        annot_path = os.path.join(self.annot_dir, self.img_files[idx].replace('.jpg', '.txt'))\n",
        "\n",
        "        # Load image\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0  # Normalize to [0, 1]\n",
        "        height, width, _ = img.shape\n",
        "\n",
        "        # Load annotations\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        with open(annot_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                cls, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "                x_min = (x_center - box_width / 2) * width\n",
        "                y_min = (y_center - box_height / 2) * height\n",
        "                x_max = (x_center + box_width / 2) * width\n",
        "                y_max = (y_center + box_height / 2) * height\n",
        "                boxes.append([x_min, y_min, x_max, y_max])\n",
        "                labels.append(int(cls))\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, bboxes=boxes, class_labels=labels)\n",
        "            img = augmented['image']\n",
        "            boxes = torch.tensor(augmented['bboxes'], dtype=torch.float32)\n",
        "            labels = torch.tensor(augmented['class_labels'], dtype=torch.int64)\n",
        "\n",
        "        # Return the image and the target dictionary\n",
        "        return torch.tensor(img).permute(2, 0, 1), {'boxes': boxes, 'labels': labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cIZE_vSlZTsC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def create_model(num_classes, grid_size=7, num_anchors=1):\n",
        "    \"\"\"\n",
        "    Creates a ResNet-based model for object detection with bounding box and class predictions.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of classes for classification.\n",
        "        grid_size (int): Size of the output grid (e.g., 7x7).\n",
        "        num_anchors (int): Number of anchors per grid cell.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The initialized ResNet-based detector model.\n",
        "    \"\"\"\n",
        "    class ResNetBoundingBoxDetector(nn.Module):\n",
        "        def __init__(self, num_classes, grid_size, num_anchors):\n",
        "            super(ResNetBoundingBoxDetector, self).__init__()\n",
        "            self.resnet = models.resnet18(pretrained=True)  # Use a pretrained ResNet\n",
        "            num_features = self.resnet.fc.in_features\n",
        "\n",
        "            # Replace the ResNet's FC layer with custom outputs\n",
        "            self.resnet.fc = nn.Identity()\n",
        "\n",
        "            # Bounding box regression: 4 coords (x, y, w, h) per anchor per grid cell\n",
        "            self.fc_bbox = nn.Linear(num_features, grid_size * grid_size * num_anchors * 4)\n",
        "\n",
        "            # Class prediction: num_classes per anchor per grid cell\n",
        "            self.fc_class = nn.Linear(num_features, grid_size * grid_size * num_anchors * num_classes)\n",
        "\n",
        "            self.sigmoid = nn.Sigmoid()  # To normalize bounding box coordinates (0-1)\n",
        "\n",
        "            self.grid_size = grid_size\n",
        "            self.num_anchors = num_anchors\n",
        "            self.num_classes = num_classes\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Feature extraction\n",
        "            x = self.resnet(x)\n",
        "\n",
        "            # Bounding box coordinates\n",
        "            bbox_coords = self.sigmoid(self.fc_bbox(x))\n",
        "            bbox_coords = bbox_coords.view(-1, self.grid_size, self.grid_size, self.num_anchors, 4)\n",
        "\n",
        "            # Class scores\n",
        "            class_scores = self.fc_class(x)\n",
        "            class_scores = class_scores.view(-1, self.grid_size, self.grid_size, self.num_anchors, self.num_classes)\n",
        "\n",
        "            return bbox_coords, class_scores\n",
        "\n",
        "    # Return an instance of the detector\n",
        "    return ResNetBoundingBoxDetector(num_classes=num_classes, grid_size=grid_size, num_anchors=num_anchors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "JD-oCFiYUYRZ",
        "outputId": "884b07e6-51c3-48b1-984a-e732e27c1739"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[4], line 40\u001b[0m, in \u001b[0;36mcreate_model.<locals>.ResNetBoundingBoxDetector.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Feature extraction\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Bounding box coordinates\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     bbox_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_bbox(x))\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Apps\\anaconda3\\envs\\yolo\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Paths\n",
        "img_dir = \"dataset/train/images\"\n",
        "annot_dir = \"dataset/train/labels\"\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = YOLODataset(img_dir, annot_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Model setup\n",
        "num_classes = 37\n",
        "model = create_model(num_classes)\n",
        "model = model.to('cuda')\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(10):  # Train for 10 epochs\n",
        "    epoch_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = list(image.to('cuda') for image in images)\n",
        "        images = torch.stack(images).to('cuda')\n",
        "        targets = [{k: v.to('cuda') for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yolo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
